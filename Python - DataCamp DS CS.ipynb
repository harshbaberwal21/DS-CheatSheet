{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36f83d52",
   "metadata": {},
   "source": [
    "# DataCamp Python\n",
    "\n",
    "#### The following contains the elements from DataCamp data science in Pythob courses, entirely new or rarely used, along with the some extras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18018bfe",
   "metadata": {},
   "source": [
    "# Basic Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fc87a8",
   "metadata": {},
   "source": [
    "### Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bae989a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d222ae1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcd'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## String Manipulation\n",
    "\n",
    "#Addition\n",
    "'ab' + 'cd'\n",
    "\n",
    "# Replace (in a dataframe here)\n",
    "df[\"string\"].str.replace(\"+\",\"00\") # regex replace r'+'\n",
    "df[\"string\"].str.len()\n",
    "df[\"string\"].str.contains(\"+|-\") ## .any() and .all() for conditionals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4706036",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To create a new list from an old one, use\n",
    "\n",
    "y = list(x)\n",
    "y = x[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe965e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Why Numpy\n",
    "\n",
    "array1.nlargest()\n",
    "array1.nsmallest()\n",
    "\n",
    "# list1 + list2 is a new list with elements of both list1 and list2\n",
    "# In numpy arrays, we can perform element wise mathematical operation between arrays. ---- np.array()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21cba54",
   "metadata": {},
   "source": [
    "### Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ff2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Keys should be immutable objects such as string, number, boolean etc. and not a mutable one such as list.\n",
    "\n",
    "dict1 = {'a_key':a_value, 'b_key': b_value}\n",
    "\n",
    "world = {\"afghanistan\":30.55, \"albania\":2.77, \"algeria\":39.21}\n",
    "print('afghanistan' in world)\n",
    "\n",
    "world['seaworld'] = 1.234\n",
    "\n",
    "## Delete a key-value pair\n",
    "del(world['seaworld'])\n",
    "world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c21af1",
   "metadata": {},
   "source": [
    "### DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabe78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## From Dictionary\n",
    "# keys (column labels)\n",
    "# values (data for a particular column)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(dict)\n",
    "\n",
    "df['column name'] ## Gives a Pandas Series\n",
    "df[['column name']] ## Gives a Pandas DataFrame\n",
    "## First goes for rows and then for columns id single slicer provided\n",
    "df[:,[]]\n",
    "df.loc[[row labels]],[columns names list]]\n",
    "df.iloc[row index : col_index]\n",
    "\n",
    "df[\"col1\"].astype('dtype name') # \"category\" for categorical data\n",
    "\n",
    "df[\"col2\"].unique()\n",
    "\n",
    "#### DataFrame Plotting\n",
    "\n",
    "## DF Histogram\n",
    "df[col_name].hist()\n",
    "\n",
    "## DF Bar Plot\n",
    "df[col_name].plot(kind=\"bar\",title=\"title\", rot = angle) ## kind = 'line','scatter' ; subplots = True/False; stacked=True for bar is there\n",
    "plt.legend([list of legends]) ## When multiple plots are plotted in one go before plt.show()\n",
    "\n",
    "## New\n",
    "pd.plotting.scatter_matrix(df, c = y, figsize = [8, 8],s=150, marker = 'D')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9542a950",
   "metadata": {},
   "source": [
    "### If Else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e99a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "z = 6\n",
    "if z % 2 == 0 :\n",
    "print(\"z is divisible by 2\") # True\n",
    "elif z % 3 == 0 :\n",
    "print(\"z is divisible by 3\") # Never reached\n",
    "else :\n",
    "print(\"z is neither divisible by 2 nor by 3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb340d",
   "metadata": {},
   "source": [
    "### While Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d134cb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 50.0\n",
    "while error > 1: # False\n",
    "error = error /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a05a839",
   "metadata": {},
   "source": [
    "### For Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2139bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in seq :\n",
    "expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ec45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in range(lb,ub,inc)\n",
    "\n",
    "## Sequence - Looping DS\n",
    "for value in list1\n",
    "for index, value in enumerate(list1)\n",
    "for c in 'string'\n",
    "for key, value in dict1.items() # DICTIONARY\n",
    "for val in np.nditer(array) # to open the NUMPY array elements one by one\n",
    "\n",
    "for label, row in df.iterrows() # To iterate over all rows of a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b628d76",
   "metadata": {},
   "source": [
    "### Extra Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f28efb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 1+1 == 2 ## Gives out nothing if true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8214eb7a",
   "metadata": {},
   "source": [
    "# Data Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa5679d",
   "metadata": {},
   "source": [
    "### DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776c0616",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic Data and Information Access and Check\n",
    "df.head() / df.tail() / df.info() / df.describe()\n",
    "\n",
    "df.shape / df.dtypes / df.values / df.columns / df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1640c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values([list of column names], ascending = [list of True/False for the columns])\n",
    "\n",
    "## For dataframe with date as dt object\n",
    "df[df[\"date_of_birth\"] > \"2015-01-01\"]\n",
    "\n",
    "df['column_name'].isin([list of values to check for ])   # Will give a boolean series\n",
    "\n",
    "## Cumulative Aggregations\n",
    "df['column_name'].cumsum() ## cummin, cumprod\n",
    "\n",
    "df.drop_duplicates(subset=[list of column names])\n",
    "\n",
    "df[\"column_name\"].value_counts(sort = True, normalize = True) # dropna = True/False\n",
    "\n",
    "df[\"column_name\"].replace(dict_mapping, inplace = True/False) ## Replacing values\n",
    "df[\"column_name\"].map(dict_mapping) ## Similar to above. Mapping new values to old ones.\n",
    "\n",
    "df.groupby([list of column names to group on])[list of columns to aggregate].mean()\n",
    "df.groupby([list of column names to group on])[list of columns to aggregate].agg([min,max etc.])\n",
    "\n",
    "df.pivot_table(values=\"vals to aggregate\", index=\"to keep in rows\", columns=\"to keep in columns\", aggfunc=[np.mean, np.median]) ## - fill_value=0, margins=True\n",
    "## aggfunc can dictionary as well. {'column_name1':'function name1','column_name2':'function name2'}\n",
    "\n",
    "df.set_index('column name') # Set column to index - Goes well with loc at times\n",
    "df.reset_index() # Remove Index - drop=True\n",
    "df.sort_index() # level = [list of index names], ascending = [list of corresponding true and false]\n",
    "## For multi-index, give tuples of lables in loc access method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bbfc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Query Method\n",
    "df.query('col_a > 90 and col_b < 140')\n",
    "df.query('col_a == \"disney\" or (col_a == \"nike\" and col_b < 90)')\n",
    "\n",
    "#### Melt Method - to Reshape data, wide to long format\n",
    "df.melt(id_vars=['col_a','col_b'],value_vars=['col_c','col_d'],var_name=['New_Var'], value_name='New_Val_Col_Name')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e458c10",
   "metadata": {},
   "source": [
    "### DataFrame - Joining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3482484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df1.merge(df2, on='key', suffixes=('_left','_right'), how = '')\n",
    "## Check shape of df afterwards / on = [list of keys]\n",
    "\n",
    "\n",
    "df= df1.merge(df2, left_on='left_key', right_on='right_key', suffixes=('_left','_right'), how = '')\n",
    "## how = left or right or inner (default) or outer\n",
    "## Additional Arguments: \n",
    "# left_index = Bool,\n",
    "# right_index = Bool, \n",
    "# indicator = Bool (indicates row availability)\n",
    "# validate = '' - one_to_one, one_to_many etc.\n",
    "\n",
    "\n",
    "#### Filtering Joins\n",
    "\n",
    "# Semi Join - Inner join with columns from left table only - No Duplicates\n",
    "# Can be done using a combination of isin and inner join using merge\n",
    "\n",
    "# Anti Join - Returns the left table excluding the intersection and no columns from right table.\n",
    "# Can be done using a combination of isin and inner join using merge\n",
    "\n",
    "\n",
    "\n",
    "#### Ordered Merge (default = Outer); to be used with - #Ordered data/time series OR #Filling in missing values\n",
    "pd.merge_ordered(df1, df2, on='key(date)', suffixes=('_left','_right'),fill_method='ffill')\n",
    "## Merge asof: Check DataCamp slides and docuentation for more on it\n",
    "pd.merge_asof(visa, ibm, on=['key(date)'], suffixes=('_left','_right'),direction='forward')\n",
    "\n",
    "\n",
    "#### Pandas CONCAT\n",
    "\n",
    "pd.concat([df1,df2,df3])\n",
    "## Additional Arguments: \n",
    "# ignore_index=True\n",
    "# keys=['df1_key','df2_key','df3_key'] # Column names not in all get NaNs\n",
    "# sort = Bool\n",
    "# join='inner' # for tables with different column names - chooses only the common ones\n",
    "# verify_integrity=False # Checks for duplicate index in the new dataframe\n",
    "\n",
    "df.append([df2,df3]) # Outer Join of Columns meaning Column names not in all get NaNs # \n",
    "## Supports: ignore_index , and sort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60514531",
   "metadata": {},
   "source": [
    "### DataFrame Missing Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna()\n",
    "df.isnull()\n",
    "df.isna().any() ## Column wise True and False indicators\n",
    "df.isna().sum() ## Column wise NA counts\n",
    "\n",
    "df.isna().sum().plot(kind=\"bar\") ## Column wise NA bar plot\n",
    "plt.show()\n",
    "\n",
    "df.dropna() # subset = [columns], inplace = True/False\n",
    "df.fillna(value)\n",
    "\n",
    "#### Move to Data Cleaning for more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaa9aca",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e148b92",
   "metadata": {},
   "source": [
    "### Basic Plots with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1517ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#### Always use the below code in the end to show the plot.\n",
    "plt.show()\n",
    "\n",
    "#### Simple plot with x and y as arrays/lists\n",
    "\n",
    "# Line\n",
    "plt.plot(x, y)\n",
    "\n",
    "# Scatter\n",
    "plt.scatter(x, y)\n",
    "\n",
    "# Histogram\n",
    "plt.hist(x, bins = n,alpha= val) ## alpha = transparency (value bw 0 and 1)\n",
    "\n",
    "## Basic Customization\n",
    "plt.xlabel('xlabel')\n",
    "plt.ylabel('ylabel')\n",
    "plt.title('title')\n",
    "plt.yticks([list of values],[list of labels])\n",
    "plt.margins(value)\n",
    "## Changing the axis scale. \n",
    "# Works with SNS too\n",
    "plt.yscale('log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824fce40",
   "metadata": {},
   "source": [
    "### More with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()  ## Gives a Figure object and an axis object\n",
    "plt.show()\n",
    "\n",
    "#### Plotting\n",
    "\n",
    "# Default kind is line\n",
    "ax.plot(x,y,kind = '', label = 'plot1', marker=\"o/v/D/*/+\", linestyle=\"--\", color=\"r\")\n",
    "\n",
    "# Scatter Plot\n",
    "ax.scatter(x,y)\n",
    "\n",
    "# Single Bar Graph\n",
    "ax.bar(x,y1)\n",
    "\n",
    "# Stack on the above Graph\n",
    "ax.bar(x,y2, bottom = y1)  # or y0 + y1 for multi stack\n",
    "\n",
    "# for a single value plot\n",
    "ax.bar('xlabel', y.mean()) # yerr = y.std(); y. might be df['col1']\n",
    "\n",
    "ax.boxplot(x,y)\n",
    "ax.hist(x,y, bins=5)\n",
    "\n",
    "\n",
    "#### Plot customization\n",
    "\n",
    "ax.set_xlabel(\"xlab\")\n",
    "ax.set_ylabel(\"ylab\")\n",
    "ax.set_title(\"title\")\n",
    "ax.legend()\n",
    "ax.set_xticklabels(list/array of values, rotation=angle) ## Mainly when X axis is categorical such as bar plot\n",
    "## Setting the Ticks \n",
    "ax.tick_params('y', colors='blue')\n",
    "## Annotating Data\n",
    "ax.annotate(\"ann_text\",xy=(x,y),xytext=(x,y),arrowprops={\"arrowstyle\":\"->\", \"color\":\"gray\"})\n",
    "# xy - position of point to annotate with ann_text\n",
    "# xytext = position of point to keep the text at\n",
    "# arrowprops={} ; a dict of parameters for the arrowprop\n",
    "\n",
    "\n",
    "#### Multiple Plots Juxtaposed\n",
    "\n",
    "## Multiple Plots in one figure\n",
    "fig, ax = plt.subplots(3, 2) # Gives us one figure object and 6 axis objects within it\n",
    "## Additional Arguments\n",
    "# sharey=True\n",
    "\n",
    "plt.show()\n",
    "ax[0, 0].plot(x,y,color='b')\n",
    "ax[0, 1].plot(x,y,color='b')\n",
    "# and so on\n",
    "\n",
    "\n",
    "#### Plotting Time Series Data\n",
    "\n",
    "# With time as index and as a dt object\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(df.index, df[\"var1\"])\n",
    "ax.plot(df.index, df[\"var2\"])\n",
    "\n",
    "## for a secondary Y Axis\n",
    "ax.plot(df.index, df[\"var1\"])\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(df.index, df[\"var2\"])\n",
    "\n",
    "\n",
    "\n",
    "#### Plot Style and Share with Matplotlib\n",
    "plt.style.use(\"ggplot\") # 'default', 'bmh', 'seaborn-colorblind', 'grayscale'\n",
    "\n",
    "fig.set_size_inches([5, 3])\n",
    "\n",
    "fig.savefig(\"image.png\") # dpi=300\n",
    "fig.savefig(\"image.jpg\") # quality=50\n",
    "fig.savefig(\"image.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a1afc",
   "metadata": {},
   "source": [
    "### Plotting with Seaborn - Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0e93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#### Basic\n",
    "\n",
    "#hue_colors = {\"Val1\":\"black\",\"Val2\":\"red\"}\n",
    "sns.scatterplot(x = \"xcol_name\", y = \"ycol_name\",data = df, hue = \"color_col_name\")\n",
    "# hue_order=[\"Val1\",\"Val2\"])\n",
    "# palette=hue_colors\n",
    "\n",
    "sns.countplot(x=\"cat_col\",data=df)\n",
    "\n",
    "\n",
    "#### Relational Plotting. relplot() - Good for multiple plots\n",
    "\n",
    "sns.relplot(x = \"xcol_name\", y = \"ycol_name\",data = df, kind=\"scatter\") # kind = 'line'\n",
    "# size=\"size_col_name\"\n",
    "# hue = \"column name\"\n",
    "# style = \"column name\"\n",
    "# dashes = False\n",
    "# alpha = 0 to 1; for transparency\n",
    "# ci = \"sd\"; Confidence Interval - Mainly for line plots - Such as sd for +- std | =None to remove\n",
    "\n",
    "#### Categorical Plotting. catplot()\n",
    "\n",
    "sns.catplot(x=\"col_name\", data=df, kind=\"count\") # kind = 'bar', 'box', 'point'\n",
    "# order = category_order; list of category names in required order\n",
    "\n",
    "\n",
    "## RelPlot and CatPlot Common Plot arguments \n",
    "# col = 'column name' ## col means columns to divide the plot into. Such as 2 for Gender, one for M and one for F.\n",
    "# row = 'column name' ## row means rows to divide the plot into. Such as 2 for Gender, one for M and one for F.\n",
    "# col_wrap = integer ## Number of plots after which to move onto to the next row in case of col\n",
    "# col_order = List of values to order the plots by\n",
    "\n",
    "\n",
    "## SNS BoxPlot\n",
    "# sym=\"\" for omitting outliers\n",
    "# Make them extend to 2.0 * IQR: whis=2.0\n",
    "# Show the 5th and 95th percentiles: whis=[5, 95]\n",
    "# Show min and max values: whis=[0, 100]\n",
    "\n",
    "# SNS Point Plot\n",
    "# join = True (if points should be joined)\n",
    "# estimator = median ; Defalt is mean just line bar plot. Here it is set to median from numpy\n",
    "# capsize = 0 to 1\n",
    "\n",
    "\n",
    "\n",
    "#### Plot Styling and Color\n",
    "\n",
    "sns.set_style('white') # \"white\", \"dark\", \"whitegrid\", \"darkgrid\", \"ticks\"\n",
    "\n",
    "sns.set_palette(\"RdBu\")\n",
    "# Diverging Palettes\n",
    "# Sequential Palettes\n",
    "# custom palette = [list of colors]\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "# sns.palettes.SEABORN_PALETTES = list of all sns palettes\n",
    "\n",
    "# Removing Axes\n",
    "sns.despine(left=True)\n",
    "\n",
    "#Setting figure context\n",
    "sns.set_context('paper') # Smallest to largest: \"paper\", \"notebook\", \"talk\", \"poster\"\n",
    "\n",
    "# Setting figure title\n",
    "g = sns.catplot(x=\"col1\",y=\"col2\", col = 'col3',data=df,kind=\"box\")\n",
    "g.fig.suptitle(\"Figure Title\", y = height)\n",
    "\n",
    "# Setting axes' labels and titles\n",
    "g.set_titles(\"This is {col_name}\") # col_name = values from column used to create the subplot in catplot\n",
    "g.set(xlabel=\"New X Label\",ylabel=\"New Y Label\")\n",
    "# xlim=(0, 50000)\n",
    "\n",
    "ax.axvline(x=x_val, label='My Line', linestyle='--') # followed by ax.legend() to shoe the line label\n",
    "\n",
    "plt.xticks(rotation=90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fe4be8",
   "metadata": {},
   "source": [
    "### Plotting with Seaborn - Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46221784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "#### Univariate Distribution Analysis Plots\n",
    "\n",
    "## The line defaults to generating a Gaussian Kernel Density Estimate (KDE)\n",
    "sns.distplot(df['col_name']) \n",
    "## Histogram\n",
    "sns.distplot(df['col_name'], kde=False, bins=10)\n",
    "## KDE + Rug Plot\n",
    "sns.distplot(df['col_name'], hist=False, rug=True) # This gives the KDE line plot with rug plot\n",
    "# kde_kws={'shade':True}\n",
    "\n",
    "## Similarly: rugplot(), kdeplot()\n",
    "\n",
    "\n",
    "\n",
    "#### Regression Analysis Plots\n",
    "\n",
    "# An axis plot\n",
    "sns.regplot(x=\"xcol\", y=\"ycol\", data=df)\n",
    "# order = integer, for polynomial regression with the order _, default = 1 \n",
    "# x_jitter=.1\n",
    "# x_estimator = np.mean ; be useful for highlighting trends\n",
    "# x_bins = Integer ; can be used to divide the data into discrete bins. Reg still fits on entire data\n",
    "# fit_reg = True ; no regression line when set to false\n",
    "\n",
    "# Faceting\n",
    "sns.lmplot(x=\"quality\",y=\"alcohol\",data=df)\n",
    "# hue = \"col_name\"\n",
    "# col = \"col_name\"\n",
    "# row = \"col_name\"\n",
    "\n",
    "# Residuals Plot\n",
    "sns.residplot(data=df, x='temp', y='total_rentals')\n",
    "# order = integer, default = 1\n",
    "\n",
    "\n",
    "\n",
    "#### Categorical Plots\n",
    "\n",
    "## Abstract Representation (One of the columns should be categorical while other numerical)\n",
    "sns.stripplot(data=df, y=\"y_col\",x=\"x_col\") \n",
    "#jitter=True\n",
    "sns.swarmplot(data=df, y=\"y_col\",x=\"x_col\")\n",
    "sns.boxplot(data=df, y=\"y_col\",x=\"x_col\")\n",
    "sns.violinplot(data=df, y=\"y_col\",x=\"x_col\")\n",
    "sns.lvplot(data=df, y=\"y_col\",x=\"x_col\")\n",
    "\n",
    "## Statistical Estimates\n",
    "sns.barplot(data=df, y=\"y_col\",x=\"x_col\")\n",
    "sns.pointplot(data=df, y=\"y_col\",x=\"x_col\")\n",
    "sns.countplot(data=df, y=\"y_col\")\n",
    "\n",
    "\n",
    "\n",
    "#### Matrix Plots\n",
    "Pandas crosstab --> HeatMap\n",
    "\n",
    "# HeatMaps\n",
    "\n",
    "df_crosstab = pd.crosstab(df[\"mnth\"], df[\"weekday\"],values=df[\"total_rentals\"], aggfunc='mean')\n",
    "sns.heatmap(df_crosstab)\n",
    "# annot=True, \n",
    "# fmt=\"d\",\n",
    "# cmap=\"YlGnBu\", \n",
    "# cbar=False, \n",
    "# linewidths=.5\n",
    "# center = df_crosstab.loc[_, _]\n",
    "sns.heatmap(df.corr())\n",
    "\n",
    "\n",
    "#### FacetGrid, FactorPlor and PairPlots\n",
    "\n",
    "## Gives out a facet grid with n axis and Mmapping a plot type.\n",
    "g = sns.FacetGrid(df, col=\"col1_name\")\n",
    "g.map(sns.boxplot, 'col2_name',order=[=list of values in order])\n",
    "\n",
    "## Above in one step\n",
    "sns.factorplot(x=\"col2_name\", data=df, col=\"col1_name\", kind='box') # Pretty much the same as catplot\n",
    "\n",
    "## PairPlot\n",
    "#Plots between col1 and col2\n",
    "sns.pairplot(df, vars=[\"col1_name\",\"col2_name\"], kind='reg',diag_kind='hist')\n",
    "# hue='col_name', \n",
    "# palette='husl',\n",
    "# plot_kws={'alpha': 0.5}; arguments for the plots\n",
    "\n",
    "\n",
    "## JointPlot\n",
    "sns.jointplot(data=df, x=\"col1\",y=\"col2\", kind='scatter')\n",
    "# xlim=(0, 25000),\n",
    "# marginal_kws=dict(bins=15,rug=True)\n",
    "\n",
    "sns.jointplot(data=df, x=\"col1\",y=\"col2\", kind='scatter').plot_joint(sns.kdeplot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc58e867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9915a629",
   "metadata": {},
   "source": [
    "## Data Importing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d759a3",
   "metadata": {},
   "source": [
    "#### Reading in flat files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a2c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### With context manager | Without it make sure to close the file\n",
    "# Text\n",
    "with open('huck_finn.txt', 'r') as file:\n",
    "print(file.read())\n",
    "\n",
    "\n",
    "#### Flat File using Numpy\n",
    "np.loadtxt(filename, delimiter=',')\n",
    "# skiprows=1\n",
    "# usecols=[0,2]\n",
    "# dtype=str\n",
    "\n",
    "\n",
    "#### Reading CSV using Pandas\n",
    "pd.read_csv(filename)\n",
    "\n",
    "data = pd.ExcelFile(\"file.xlsx\")\n",
    "print(data.sheet_names)\n",
    "df1 = data.parse('SheetName') # sheet name, as a string\n",
    "df2 = data.parse(0) # sheet index, as a float\n",
    "\n",
    "\n",
    "\n",
    "#### Using iterators to load large files into memory\n",
    "## read_csv argument: chunksize\n",
    "result = []\n",
    "for chunk in pd.read_csv('data.csv', chunksize=1000):\n",
    "    result.append(sum(chunk['x']))\n",
    "total = sum(result)\n",
    "print(total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16006578",
   "metadata": {},
   "source": [
    "#### Reading in Other files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52258f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Importing Pickled Files\n",
    "import pickle\n",
    "with open('pickled_fruit.pkl', 'rb') as file:\n",
    "data = pickle.load(file)\n",
    "print(data)\n",
    "\n",
    "\n",
    "\n",
    "#### Importing SAS files\n",
    "from sas7bdat import SAS7BDAT\n",
    "with SAS7BDAT('urbanpop.sas7bdat') as file:\n",
    "df_sas = file.to_data_frame()\n",
    "\n",
    "#### Importing Stata files\n",
    "data = pd.read_stata('urbanpop.dta')\n",
    "\n",
    "\n",
    "\n",
    "#### Importing HDF5 files\n",
    "import h5py\n",
    "filename = 'H-H1_LOSC_4_V1-815411200-4096.hdf5'\n",
    "data = h5py.File(filename, 'r') # 'r' is to read\n",
    "print(type(data))\n",
    "\n",
    "for key in data.keys():\n",
    "print(key)\n",
    ">> meta\n",
    ">> quality\n",
    ">> strain\n",
    "\n",
    "\n",
    "\n",
    "#### Importing Matlab Files\n",
    "import scipy.io\n",
    "filename = 'workspace.mat'\n",
    "mat = scipy.io.loadmat(filename)\n",
    "\n",
    "# keys = MATLAB variable names\n",
    "# values = objects assigned to variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da35b79",
   "metadata": {},
   "source": [
    "#### Reading in from Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a95746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SQLITE\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "\n",
    "table_names = engine.table_names()\n",
    "print(table_names)\n",
    "\n",
    "import pandas as pd\n",
    "con = engine.connect()\n",
    "rs = con.execute(\"Query\")\n",
    "\n",
    "df = pd.DataFrame(rs.fetchall()) # / (rs.fetchmany(size=5)\n",
    "df.columns = rs.keys()\n",
    "\n",
    "con.close()\n",
    "\n",
    "\n",
    "OR \n",
    "\n",
    "with engine.connect() as con:\n",
    "rs = con.execute(\"Query\")\n",
    "df = pd.DataFrame(rs.fetchmany(size=5))\n",
    "df.columns = rs.keys()\n",
    "\n",
    "OR\n",
    "\n",
    "df = pd.read_sql_query(\"Query\", engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c5a8ad",
   "metadata": {},
   "source": [
    "#### Reading in From Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ddc142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Automatic file download in Python\n",
    "from urllib.request import urlretrieve\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
    "urlretrieve(url, 'winequality-white.csv')\n",
    "\n",
    "\n",
    "\n",
    "#### GET requests using urllib\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "url = \"https://www.wikipedia.org/\"\n",
    "#Create Request\n",
    "request = Request(url)\n",
    "#Get Response\n",
    "response = urlopen(request)\n",
    "#Read from Response\n",
    "html = response.read()\n",
    "#Close Response\n",
    "response.close()\n",
    "\n",
    "\n",
    "\n",
    "#### GET requests using requests\n",
    "import requests\n",
    "url = \"https://www.wikipedia.org/\"\n",
    "r = requests.get(url)\n",
    "text = r.text\n",
    "\n",
    "\n",
    "#### BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://www.crummy.com/software/BeautifulSoup/'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc)\n",
    "\n",
    "soup.prettify()\n",
    "soup.title\n",
    "soup.get_text()\n",
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d06cf",
   "metadata": {},
   "source": [
    "#### JSONs and APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Reading a json file\n",
    "import json\n",
    "with open('snakes.json', 'r') as json_file:\n",
    "    json_data = json.load(json_file)              ## Gives out a dictionary of values\n",
    "\n",
    "for key, value in json_data.items():\n",
    "print(key + ':', value)\n",
    "\n",
    "\n",
    "#### Connecting to an API in Python\n",
    "\n",
    "import requests\n",
    "url = 'http://www.omdbapi.com/?t=hackers'   # \"?t=hackers\" is the query\n",
    "r = requests.get(url)\n",
    "json_data = r.json() # Getting the webppage in the json\n",
    "\n",
    "for key, value in json_data.items():\n",
    "    print(key + ':', value)\n",
    "\n",
    "#### Connecting to twitter API\n",
    "## View the datacamp slides for this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df356c9",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "1. Data Type Constraints. They should conform to the information type.\n",
    "2. Data Range Constraints. Check the values against logical range of data. Remove/Impute oddities.\n",
    "3. Uniqueness constraints. What data should be unique and checking that against many levels. Remove or Group for uniqueness.\n",
    "4. Membership Constraints. When a certain data column can only take from a finite number of values.\n",
    "\n",
    "#### Uniformity\n",
    "1. Check for values that are way out of the expected range and try to reason them if possible.\n",
    "2. Make sure the date columns are stored as datetime objects. If string, convert and make sure they are uniform.\n",
    "    Leave NA for ambiguity such as don't know the date. Impute NAs or today's date for date > today's date\n",
    "\n",
    "#### Categorical Variables\n",
    "1. Value inconsistency; Inconsistent â€€elds: 'married' , 'Maried' , 'UNMARRIED' , 'not married'\n",
    "2. Collapsing too many categories to few. - Create categories from bins or map few cats to many\n",
    "3. Making sure data is of type category. \n",
    "\n",
    "#### Cleaning text Data\n",
    "1. Data inconsistency\n",
    "2. Fixed Length Violations. Such as phone numbers.\n",
    "3. Typos\n",
    "\n",
    "#### Cross field validation\n",
    "    If there is scope for it. Such as when individuals columns should add up to a totals column.\n",
    "\n",
    "#### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4006d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col1'].astype(\"dtype\")\n",
    "\n",
    "#### Duplicate Check on Dataframe\n",
    "duplicates = df.duplicated(subset = column_names_list) ## Gives out a boolean\n",
    "#keep = False\n",
    "df[duplicates].sort_values(by = 'col_name')\n",
    "\n",
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "\n",
    "#### Membership Constraints - Getting incosistent categories for a column\n",
    "inconsistent_categories = set(df['cat_col']).difference(categories['cat_col_possible'])\n",
    "print(inconsistent_categories)\n",
    "\n",
    "inconsistent_rows = study_data['blood_type'].isin(inconsistent_categories)\n",
    "study_data[inconsistent_rows]\n",
    "\n",
    "\n",
    "\n",
    "#### Collapsing data into categories\n",
    "## Create categories out of data:\n",
    "# Using cut() - create category ranges and names\n",
    "ranges = [0,200,500,np.inf]\n",
    "group_names = ['0-200', '200-500', '500+']\n",
    "# Create income group column\n",
    "df['income_group'] = pd.cut(df['income'], bins=ranges,labels=group_names)\n",
    "\n",
    "## Map categories to fewer ones\n",
    "mapping = {'Microsoft':'DesktopOS', 'MacOS':'DesktopOS', 'Linux':'DesktopOS','IOS':'MobileOS', 'Android':'MobileOS'}\n",
    "devices['operating_system'] = devices['operating_system'].replace(mapping) # Replace dict - 'old':'new'\n",
    "devices['operating_system'].unique()\n",
    "\n",
    "\n",
    "\n",
    "#### Missing Values Check\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize missingness ## Use after describe and sorting accordingly to check uncover any pattern in missingness.\n",
    "msno.matrix(df)\n",
    "plt.show()\n",
    "\n",
    "## Dropping NAs\n",
    "df = df.dropna(subset = [list of column names])\n",
    "df = df.fillna({'col1': col_mean}) # Can impute using median etc.\n",
    "\n",
    "\n",
    "\n",
    "#### String Comparison\n",
    "\n",
    "## Using: Levenshtein: insertion, substitution, deletion\n",
    "from fuzzywuzzy import fuzz\n",
    "# Compare reeding vs reading\n",
    "fuzz.WRatio('Reeding', 'Reading')\n",
    "\n",
    "# Import process\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Define string and array of possible matches\n",
    "string = \"Houston Rockets vs Los Angeles Lakers\"\n",
    "choices = pd.Series(['Rockets vs Lakers', 'Lakers vs Rockets','Houson vs Los Angeles', 'Heat vs Bulls'])\n",
    "process.extract(string, choices, limit = 2)\n",
    "\n",
    "#### FuzzyWuzzy Collapse All\n",
    "## Check DataCamp Slides for the code and more\n",
    "\n",
    "#### Record Linkage\n",
    "## Check DataCamp Slides for details and code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b273c36",
   "metadata": {},
   "source": [
    "## Date and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25aa7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Basic Dates\n",
    "\n",
    "## Import date\n",
    "from datetime import date\n",
    "\n",
    "## Create dates\n",
    "two_hurricanes_dates = [date(2016, 10, 7), date(2017, 6, 21)]\n",
    "\n",
    "two_hurricanes_dates[0].year # .month, .day, weekday() (0,M - 6,S)\n",
    "\n",
    "## Minimum of Dates\n",
    "min(two_hurricanes_dates)\n",
    "\n",
    "## Subtract two dates\n",
    "delta = d2 - d1\n",
    "delta.days # delta (=timedelta) in days\n",
    "\n",
    "## Creating timedelta and working with Duration\n",
    "from datetime import timedelta\n",
    "td = timedelta(days=2, seconds=3) # weeks/months etc. Can add this delta to any date now. Value can be a negative integer too.\n",
    "\n",
    "duration = end - start # both are datetime objects\n",
    "duration.total_seconds()\n",
    "\n",
    "\n",
    "\n",
    "#### Dates Into Strings\n",
    "d = date(2017, 11, 5)\n",
    "d.isoformat()  # for date in ISO 8601 format\n",
    "d.strftime(\"%Y/%m/%d\") ## Any text in the middle or wherever works\n",
    "dt.strftime(\"%Y-%m-%d %H:%M:%S\") OR dt.strftime(\"%Y-%m-%d on %H:%M:%S\")\n",
    "\n",
    "\n",
    "#### Datetime\n",
    "from datetime import datetime\n",
    "dt = datetime(year=2017, month=10, day=1, hour=15, minute=23, second=25, microsecond=500000)\n",
    "\n",
    "## Replace parts of datetime\n",
    "dt_hr = dt.replace(minute = 0, second = 0, microsecond = 0)\n",
    "\n",
    "\n",
    "#### Parsing strings as datetimes\n",
    "from datetime import datetime\n",
    "dt = datetime.strptime(\"12/30/2017 15:19:13\",\"%m/%d/%Y %H:%M:%S\")\n",
    "\n",
    "\n",
    "#### FOR UTC OFFSETS look through datacamp slides\n",
    "\n",
    "\n",
    "\n",
    "#### Date and time data in Pandas\n",
    "\n",
    "## Read CSV with parsing dates\n",
    "df = pd.read_csv('file.csv',parse_dates = ['date col 1', 'date col 2'])\n",
    "OR\n",
    "df['date col 1'] = pd.to_datetime(rides['date col 1'],format = \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "## Access datetime methods and attributes by using .dt\n",
    "\n",
    "df['date col 1'].dt.weekday_name # .year\n",
    "\n",
    "rides['Duration'] = rides['End date'] - rides['Start date'] ## Duration is as time delta column\n",
    "rides['Duration'].dt.total_seconds().head(5)\n",
    "rides['Duration'].dt.total_seconds().min() # \n",
    "rides['Duration'].mean() # .sum() OR .sum()/timedelta(days = total_number) for percentage duration use.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Resample on Month / Can use groupby as well\n",
    "# Average duration by month\n",
    "rides.resample('M', on = 'Start date')['Duration seconds'].mean()\n",
    "\n",
    "## Shifts the column/dates by one row\n",
    "rides['End date'].shift(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bfdaf7",
   "metadata": {},
   "source": [
    "# Data Science Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1109d103",
   "metadata": {},
   "source": [
    "## User Defined Functions\n",
    "\n",
    "Go through the slides of Datacamp course on writing python functions  for more on doc strings, context managers, and functions as objects.\n",
    "\n",
    "Defining Functions, with multiple arguments, returning multiple values and understanding lambda functions along with map function.\n",
    "\n",
    "##### Scopes searched (in order)\n",
    "1. Local scope\n",
    "2. Enclosing functions\n",
    "3. Global\n",
    "4. Built-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58624b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raise_both(value1, value2):\n",
    "    \"\"\"Raise value1 to the power of value2 and vice versa.\"\"\"\n",
    "    new_value1 = value1 ** value2\n",
    "    new_value2 = value2 ** value1\n",
    "    new_tuple = (new_value1, new_value2)\n",
    "    return new_tuple\n",
    "\n",
    "\n",
    "\n",
    "#### Flexible Arguments\n",
    "\n",
    "## Args\n",
    "def add_all(*args):\n",
    "    \"\"\"Sum all values in *args together.\"\"\"\n",
    "    sum_all = 0\n",
    "    # Accumulate the sum\n",
    "    for num in args:\n",
    "        sum_all += num\n",
    "    return sum_all\n",
    "\n",
    "## kwargs\n",
    "def print_all(**kwargs):\n",
    "    \"\"\"Print out key-value pairs in **kwargs.\"\"\"\n",
    "    # Print out the key-value pairs of the kwargs dictionary\n",
    "    for key, value in kwargs.items():\n",
    "        print(key + \" : \" + value)\n",
    "\n",
    "\n",
    "\n",
    "#### Lambda Functions\n",
    "raise_to_power = lambda x, y: x ** y\n",
    "raise_to_power(2, 3)\n",
    "\n",
    "\n",
    "## Map.\n",
    "map(func, seq)\n",
    "funced = map(lambda num: num ** 2, nums) \n",
    "list(funced)\n",
    "# nums is a list of numbers. \n",
    "## This gives out a map object. Which can be converted to list using list()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Error Handling with try-except and raise\n",
    "\n",
    "def sqrt(x):\n",
    "    if x < 0:\n",
    "        raise ValueError('x must be non-negative')\n",
    "    try:\n",
    "        return x ** 0.5\n",
    "    except TypeError:\n",
    "        print('x must be an int or float')\n",
    "\n",
    "sqrt(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5261541",
   "metadata": {},
   "source": [
    "## Iterators\n",
    "\n",
    "##### To start with, check the for loop in basics\n",
    "\n",
    "Iterable : An object with an asociated iter() method. Applying iter() to an iterable creates an iterator \n",
    " \n",
    "Iterator: Produces next value with next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41f14e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'Da'\n",
    "it = iter(word)\n",
    "next(it)\n",
    ">>D\n",
    "next(it)\n",
    ">>a\n",
    "\n",
    "print(*iterator) ## Gives out all the values at once\n",
    "\n",
    "\n",
    "#### Iterating over file connections\n",
    "file = open('file.txt')\n",
    "it = iter(file)\n",
    "print(next(it)) ## Gives out one line at a time\n",
    "\n",
    "#### Enumerate\n",
    "## This gives an iterator of tupples of list item index and value.\n",
    "e = enumerate(list_123)\n",
    "print(list(e)) ## Print a list of tupples\n",
    "\n",
    "## Unpacking\n",
    "for index, value in enumerate(list_123): ## enumerate argument: start=n\n",
    "    print(index, value)\n",
    "\n",
    "\n",
    "\n",
    "#### Zip\n",
    "## Zip two lists\n",
    "## Gives out a zip class object that iterates over the tupples of same index values of the two lists\n",
    "z = zip(l1,l2) ## or print(*z) to print all the tupples side by side\n",
    "## Unpack\n",
    "for z1, z2 in zip(avengers, names):\n",
    "    print(z1, z2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9773cb",
   "metadata": {},
   "source": [
    "## List and Dict Comprehensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afcdc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### List Comprehensions\n",
    "\n",
    "## Eg 1\n",
    "nums = [12, 8, 21, 3, 16]\n",
    "new_nums = [num + 1 for num in nums]\n",
    "\n",
    "## Eg 2\n",
    "[num for num in range(11)]\n",
    "\n",
    "## Eg 3 - With two variables | Tradeoff: readability\n",
    "[(num1, num2) for num1 in range(0, 2) for num2 in range(6, 8)]\n",
    "\n",
    "## Eg 4 - with conditionals\n",
    "[num ** 2 for num in range(10) if num % 2 == 0]  # This will truncate the list (range(10) = 0 to 9 - 10 digits)\n",
    "\n",
    "## Eg 5 - Conditionals on the output expression\n",
    "[num ** 2 if num % 2 == 0 else 0 for num in range(10)] # This will not truncate anything.\n",
    "\n",
    "\n",
    "\n",
    "#### Dict Comprehensions\n",
    "## Create dictionaries\n",
    "## Use curly braces {} instead of brackets []\n",
    "pos_neg = {num: -num for num in range(9)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6606689d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 6), (0, 7), (1, 6), (1, 7)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(num1, num2) for num1 in range(0, 2) for num2 in range(6, 8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de69bd6",
   "metadata": {},
   "source": [
    "## Generator Expressions\n",
    "At times easy on memory than the list comprehensions. Because things are not really loaded in the workspace.\n",
    "\n",
    "##### List comprehensions vs. generators\n",
    "1. List comprehension - returns a list\n",
    "2. Generators - returns a generator object\n",
    "3. Both can be iterated over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabd7a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Use () instead of [] for a generator expression\n",
    "result = (2 * num for num in range(10)) ## result is a generator object\n",
    "\n",
    "result = (num for num in range(6))\n",
    "for num in result:\n",
    "    print(num)\n",
    "\n",
    "list(result) ## Gives a list of all the generator object sequence values.\n",
    "\n",
    "## Lazy evaluation\n",
    "print(next(result)) # Gives out all the values one by one. One value per call till it hits the last one.\n",
    "\n",
    "\n",
    "\n",
    "#### Defining a Generator Function\n",
    "\n",
    "def num_sequence(n):\n",
    "    \"\"\"Generate values from 0 to n.\"\"\"\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        yield i\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60cfeed",
   "metadata": {},
   "source": [
    "## EDA in Python\n",
    "\n",
    "Important Steps and some new functions and ideas\n",
    "\n",
    "1. Understand the data using the data code book or a data dictionary. A document with all the information on the data, its type and eccentricities.\n",
    "\n",
    "2. Use Value Counts to compare a variable between two groups\n",
    "\n",
    "\n",
    "\n",
    "#### PMF vs CDF\n",
    "##### If you draw a random element from a distribution:\n",
    "1. PMF (Probability Mass Function) is the probability that you get exactly x\n",
    "2. CDF (Cumulative Distribution Function) is the probability that you get a value <= x\n",
    "  \n",
    "  for a given value of x.\n",
    "\n",
    "- PMF over histogram\n",
    "- Use KDE if there are a lot of values\n",
    "- CDF better for comparing distributions (for eg. same attributes over two time periods)\n",
    "\n",
    "Regression is not causation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e4e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### High Level Data Check\n",
    "\n",
    "df.shape\n",
    "df.columns\n",
    "\n",
    "df.info()\n",
    "df.head()\n",
    "df.tail()\n",
    "df.describe()\n",
    "df.value_counts().sort_index()\n",
    "\n",
    "## You can sum and mean a boolean series.\n",
    "\n",
    "## Making Chnages\n",
    "pounds.replace([Val1, Val2], np.nan, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#### Univariate Analysis\n",
    "\n",
    "## Visualize the data - Histograms for Univariate analysis\n",
    "plt.hist(df_col.dropna(), bins=30)\n",
    "\n",
    "## PMF for discrete (PDF for continuous)\n",
    "pmf_df = Pmf(df, normalize=False)\n",
    "pmf_df.head()\n",
    "pmf_df.bar(label='label')\n",
    "plt.show()\n",
    "\n",
    "## CDF\n",
    "cdf = Cdf(df['col'])\n",
    "cdf(val1) # Gives out the percentage of values less than val1 \n",
    "cdf.inverse(prob) # prob is percentile basically. 0.75 for 75th percentile.\n",
    "cdf.plot()\n",
    "plt.show()\n",
    "\n",
    "## Comparing with normal CDF\n",
    "xs = np.linspace(-3, 3)\n",
    "ys = norm(0,1).pdf(xs)\n",
    "plt.plot(xs, ys, color='gray')\n",
    "sns.kdeplot(sample)\n",
    "\n",
    "\n",
    "\n",
    "#### Analyzing Relationships\n",
    "\n",
    "## Good Scatter Plot\n",
    "# Transparency --> Marker Size --> Jittering both x and y --> Zoom (labels code not added here)\n",
    "height_jitter = height + np.random.normal(0, 2, size=len(brfss))\n",
    "weight_jitter = weight + np.random.normal(0, 2, size=len(brfss))\n",
    "plt.plot(height_jitter, weight_jitter, 'o', markersize=1, alpha=0.01)\n",
    "plt.axis([x_in, x_last, y_in, y_last])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## Using Box and Violin Plots to check for any relationship\n",
    "\n",
    "## Correlation\n",
    "# Only comment on linear relationship. Can't uncover any non-linear relationship\n",
    "df.corr()\n",
    "\n",
    "\n",
    "## Simple Linear Regression\n",
    "from scipy.stats import linregress\n",
    "res = linregress(xs, ys)\n",
    "\n",
    "# Can also use\n",
    "slope, intercept = np.polyfit(y, x, 1) # degree = 1 here, can be set to 2 for quadratic regression and so on\n",
    "\n",
    "# Regression Line\n",
    "# When Plotting regression lines, with 2 features (x1 and x2), plot multiple lines for several points of x1 in a plot of x2 and y \n",
    "fx = np.array([xs.min(), xs.max()])\n",
    "fy = res.intercept + res.slope * fx\n",
    "plt.plot(fx, fy, '-')\n",
    "\n",
    "## Quick Statistical Multiple Regression\n",
    "import statsmodels.formula.api as smf\n",
    "model = smf.ols('INCOME2 ~ _VEGESU1', data=brfss)\n",
    "results = model.fit()\n",
    "results.params\n",
    "result.predict()\n",
    "\n",
    "## Quick Logistic Regression\n",
    "\n",
    "formula = 'gunlaw ~ age + age2 + educ + educ2 + C(sex)'\n",
    "results = smf.logit(formula, data=gss).fit()\n",
    "results.params\n",
    "results.predict(df) ## gives out the probability\n",
    "## Look at slides to know how to better plot this\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdcc4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04848001",
   "metadata": {},
   "source": [
    "## Statistics\n",
    "\n",
    "Optimal parameters\n",
    ": Parameter values that bring the model in closest agreement with the data\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "p-value\n",
    ": The probability of obtaining a value of your test statistic that is at least as extreme as what was observed, under the assumption the null hypothesis is true.\n",
    "\n",
    "Your statistical thinking skills\n",
    "- Perform EDA\n",
    "    - Generate effective plots like ECDFs\n",
    "    - Compute summary statistics\n",
    "- Estimate parameters\n",
    "    - By optimization, including linear regression\n",
    "    - Determine con,dence intervals\n",
    "- Formulate and test hypotheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecbae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Numpy Basic Stats\n",
    "np.mean()\n",
    "np.median()\n",
    "np.percentile(df['col'], [25, 50, 75])\n",
    "\n",
    "np.var\n",
    "np.std()\n",
    "\n",
    "np.corrcoef()\n",
    "\n",
    "np.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e018b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Numbers using Numpy\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "np.random.rand\n",
    "np.random.random()\n",
    "np.random.randint(lb,up)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f694bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Steps to check stats\n",
    "\n",
    "## Generating Histogram | Use sns.set() for better visualizations\n",
    "## Build swarmplots / violin plots / boxplots / strip-plots to check the distribution\n",
    "\n",
    "## Building ECDFs | Check EDA | Check below for another trick for ECDF\n",
    "x = np.sort(df_swing['dem_share'])\n",
    "y = np.arange(1, len(x)+1) / len(x)\n",
    "_ = plt.plot(x, y, marker='.', linestyle='none')\n",
    "\n",
    "## Check Sample Statistics such as mean/median by groups and more\n",
    "## Outlier check - Huge gap between mean and median - BOX PLOT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f4932a",
   "metadata": {},
   "source": [
    "### Statistics Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e67bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hypothesis Tests\n",
    "\n",
    "stats.ttest_1samp\n",
    "stats.ttest_ind\n",
    "stats.ttest_rel\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d2329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7844148",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6729e73",
   "metadata": {},
   "source": [
    "## Hacker Statistics\n",
    "\n",
    "Bootstrapping\n",
    ": The use of resampled data to perform statistical inference\n",
    "\n",
    "Bootstrap sample\n",
    ": A resampled array of the data\n",
    "\n",
    "Bootstrap replicate\n",
    ": A statistic computed from a resampled array\n",
    "\n",
    "\n",
    "\n",
    "**Pipeline for hypothesis testing**\n",
    "1. Clearly state the null hypothesis\n",
    "2. Define your test statistic\n",
    "3. Generate many sets of simulated data assuming the null hypothesis is true\n",
    "5. Compute the test statistic for each simulated data set\n",
    "6. The p-value is the fraction of your simulated data sets for which the test statistic is at least as extreme as for the real data\n",
    "\n",
    "\n",
    "\n",
    "One sample test\n",
    ": Compare one set of data to a single number\n",
    "\n",
    "Two sample test\n",
    ": Compare two sets of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512b7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Random Number Generators\n",
    "\n",
    "samples = np.random.binomial(trials, probability of success, size=10000)\n",
    "samples = np.random.poisson(avg_rate, size=10000)\n",
    "samples = np.random.exponential(mean, size=10000)\n",
    "samples = np.random.normal(mean, std, size=10000)\n",
    "\n",
    "\n",
    "## Checking for optimal parameters\n",
    "mean = np.mean(michelson_speed_of_light)\n",
    "std = np.std(michelson_speed_of_light)\n",
    "samples = np.random.normal(mean, std, size=10000)\n",
    "\n",
    "## Plot theoretical normal PDF against the PDF to get analyze the distribution\n",
    "\n",
    "\n",
    "#### Bootstrapping\n",
    "\n",
    "np.random.choice([1,2,3,4,5], size=5) ## resamples the given array with replacement to generate a 5-dim array\n",
    "\n",
    "## The following function gives out a single bootstrap replicate of the given function.\n",
    "# For many (say 10,000), loop over it and save result.\n",
    "def bootstrap_replicate_1d(data, func):\n",
    "    \"\"\"Generate bootstrap replicate of 1D data.\"\"\"\n",
    "    bs_sample = np.random.choice(data, len(data))\n",
    "    return func(bs_sample)\n",
    "\n",
    "bootstrap_replicate_1d(michelson_speed_of_light, np.mean)\n",
    "\n",
    "bs_replicates = np.empty(10000)\n",
    "for i in range(10000):\n",
    "    bs_replicates[i] = bootstrap_replicate_1d(df['col'], np.mean)\n",
    "## bs_replactes should have a normal distribution.\n",
    "\n",
    "## Bootstrap confidence interval\n",
    "conf_int = np.percentile(bs_replicates, [2.5, 97.5])\n",
    "\n",
    "\n",
    "## Pairs bootstrapping. \n",
    "# Such as for regression lines.\n",
    "# Use np.random.choice over index and then sample pair of values using the indices.\n",
    "\n",
    "\n",
    "\n",
    "#### Hypothesis Simulation\n",
    "## Check the datacamp slides for more on this and\n",
    "\n",
    "# Bootstrap hypothesis tests\n",
    "# A/B Test\n",
    "# One and Two sample Tests\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd298ab",
   "metadata": {},
   "source": [
    "#### DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284a62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Stats/Aggregation\n",
    "df['column_name'].mean() ## Can provide axis as well.\n",
    "## .median() , .mode(), .min() , .max(), .var() , .std(), .sum(), .quantile(0.3)\n",
    "\n",
    "df[[\"col_a\", \"col_b\"]].agg(function to aggregate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a7e71",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "#### Read basic theory, info adn important points from datacamp slides. Quick and easy read.\n",
    "\n",
    "**Scikit-learn fit and predict**\n",
    "- All machine learning models implemented as Python classes\n",
    "    - They implement the algorithms for learning and predicting\n",
    "    - Store the information learned from the data\n",
    "- Training a model on the data = â€˜,â€€ingâ€™ a model to the data\n",
    "    - .fit() method\n",
    "- To predict the labels of new data: .predict() method\n",
    "\n",
    "In SKlearn - Instantiate the model --> fit on training data --> predict test/train --> score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c29a4c6",
   "metadata": {},
   "source": [
    "### Preprocessing and Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4133942",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Label Encoder\n",
    "\n",
    "## One Hot Encoding - For n categories, we need n-1 binary features to encode all information\n",
    "df_onehot = pd.get_dummies(df)\n",
    "\n",
    "## Missing Value Imputer\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='mean', axis=0) ## With axis =0 strategy works across rows, i.e. column means\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)\n",
    "\n",
    "\n",
    "## Scaling\n",
    "from sklearn.preprocessing import scale\n",
    "X_scaled = scale(X)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler ## More Such: MaxAbsScaler, Normalizer\n",
    "scaler_1 = StandardScaler()  ## copy=True, with_mean=True, with_std=True\n",
    "scaler_1.fit(X) ## .transform() or .fit_transform()\n",
    "\n",
    "\n",
    "\n",
    "#### CountVectorizer\n",
    "## can use CountVectorizer() directly in pipleline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "TOKENS_BASIC = '\\\\\\\\S+(?=\\\\\\\\s+)'\n",
    "df.text_column.fillna('', inplace=True)\n",
    "vec_basic = CountVectorizer(token_pattern=TOKENS_BASIC) ## ngram_range=(1, 2)\n",
    "\n",
    "\n",
    "#### Interaction Terms Using Polynomial Features class\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "interaction = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "interaction.fit_transform(x)\n",
    "\n",
    "## For Sparse Matrix (csr_matrix)\n",
    "SparseInteractions(degree=2).fit_transform(x).toarray()\n",
    "\n",
    "\n",
    "\n",
    "#### Hashing | Memory Efficient\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "vec = HashingVectorizer(norm=None,non_negative=True,token_pattern=TOKENS_ALPHANUMERIC,ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2134ad",
   "metadata": {},
   "source": [
    "### Model Selection and Metrics\n",
    "\n",
    "- Plot regression output\n",
    "- Cross Validate\n",
    "\n",
    "- Classification\n",
    "    - Confusion matrix along with Precision, Recall and F1 Score\n",
    "    - Larger area under the ROC curve = better model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac86c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21) # ,stratify=y\n",
    "\n",
    "\n",
    "## Simple Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv_results = cross_val_score(estimator, X, y, cv=5, n_jobs = -1) # estimator = model ## Gives out array of 5 scores\n",
    "# scoring='roc_auc' / 'neg_mean_squared_error' / \n",
    "# Set n_jobs to -1 in order to exploit all CPU cores in computation\n",
    "\n",
    "## Cross Validate - Extensive\n",
    "from sklearn.model_selection import cross_validate\n",
    "cross_validate(lasso, X, y, cv=3)\n",
    "#scoring=('r2', 'neg_mean_squared_error'),\n",
    "#return_train_score=True\n",
    "\n",
    "\n",
    "#### Hyperparameter Tuning\n",
    "\n",
    "## Grid Search\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'n_neighbors': np.arange(1, 50)}\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(knn, param_grid, cv=5)\n",
    "knn_cv.fit(X, y)\n",
    "\n",
    "knn_cv.best_params_\n",
    "knn_cv.best_score_\n",
    "knn_cv.best_estimator_\n",
    "\n",
    "## \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Classification Metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, y_pred) ## y_pred comes from predct method of the classifier/estimator.\n",
    "classification_report(y_test, y_pred)\n",
    "\n",
    "# ROC AUC Score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "\n",
    "## Accuracy Score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "## Mean Squared Error \n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ee614",
   "metadata": {},
   "source": [
    "### Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072e0b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### KNN Classifier\n",
    "## High n_neighbors, more overfitting\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(X, y)\n",
    "knn.predict(X_test)\n",
    "knn.score(X_test, y_test) ## Gives out accuracy\n",
    "\n",
    "\n",
    "#### Regression\n",
    "## Fitting model and plotting the line\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "prediction_space = np.linspace(min(X),max(X)).reshape(-1, 1)\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(prediction_space, reg.predict(prediction_space),color='black', linewidth=3)\n",
    "plt.show()\n",
    "\n",
    "reg.score(X_test, y_test) # R Squared\n",
    "\n",
    "\n",
    "\n",
    "#### Regularized Regression\n",
    "\n",
    "## Ridge (Squared Coeff in Loss)\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge(alpha=0.1, normalize=True)\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge_pred = ridge.predict(X_test)\n",
    "ridge.score(X_test, y_test)\n",
    "\n",
    "\n",
    "## Lasso (Absolute Coeff in Loss)\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso(alpha=0.1, normalize=True)\n",
    "lasso.fit(X_train, y_train)\n",
    "lasso_pred = lasso.predict(X_test)\n",
    "lasso.score(X_test, y_test)\n",
    "\n",
    "\n",
    "## Lasso Regression for feature selection (Shrinks the coefficients of less important features to exactly 0)\n",
    "## By plotting coefficients of lasso model for all the features.\n",
    "\n",
    "\n",
    "#### Logistic regression \n",
    "## for binary classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_pred_prob = logreg.predict_proba(X_test) ## Gives out the predicted probabilities\n",
    "\n",
    "## Plotting ROC Curve\n",
    "# check the Datacamp slides for the code\n",
    "\n",
    "\n",
    "#### One Vs Rest Classification\n",
    "## Fitting one classifier each for every column in y_train\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "clf = OneVsRestClassifier(LogisticRegression())\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb6e8aa",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "Finds Patterns in data\n",
    "- Clustering\n",
    "- Data Compression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d74bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### K-Means\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(samples)\n",
    "\n",
    "labels = model.predict(samples)\n",
    "\n",
    "## Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "xs = samples[:,0]\n",
    "ys = samples[:,2]\n",
    "plt.scatter(xs, ys, c=labels)\n",
    "plt.show()\n",
    "\n",
    "## For Evaluation\n",
    "model.inertia_\n",
    "\n",
    "\n",
    "\n",
    "#### Hierarchical Clustering\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "## Building the distance matrix and the dendrogram. \n",
    "mergings = linkage(samples, method='complete') ## method = single, ward\n",
    "dendrogram(mergings,labels=country_names,leaf_rotation=90,leaf_font_size=6)\n",
    "plt.show()\n",
    "## Clustering\n",
    "labels = fcluster(mergings, 15, criterion='distance') ## n_clusters = 15 here\n",
    "print(labels)\n",
    "\n",
    "\n",
    "\n",
    "#### t-SNE for 2-dimensional maps ## Check datacamp slides for more info\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(learning_rate=100) ## learning rate: Wrong choice = points bunch together | try values between 50 and 200\n",
    "transformed = model.fit_transform(samples)\n",
    "xs = transformed[:,0]\n",
    "ys = transformed[:,1]\n",
    "plt.scatter(xs, ys, c=species)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### PCA\n",
    "\n",
    "## De-Correlating the features\n",
    "from sklearn.decomposition import PCA\n",
    "model = PCA()\n",
    "\n",
    "model.fit(samples)\n",
    "transformed = model.transform(samples)\n",
    "\n",
    "model.components_\n",
    "features = range(model.n_components_)\n",
    "\n",
    "model.explained_variance_\n",
    "\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "\n",
    "## Dimension Reduction. \n",
    "# After one run, chek the components for maximum variance and run PCA again with that number of components.\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca.fit(samples)\n",
    "transformed = pca.transform(samples)\n",
    " \n",
    "## PCA with Sparse arrays and csr_matrix (scipy.sparse.csr_matrix)\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "model = TruncatedSVD(n_components=3)\n",
    "model.fit(documents) # documents is csr_matrix\n",
    "transformed = model.transform(documents)\n",
    "\n",
    "\n",
    "\n",
    "#### Non-Negative Matrix Factorization\n",
    "## Only for non-negative metrics\n",
    "## Works with both numpy arrays and csr_matrix\n",
    "## Interpretable using the components. Dimension of a component = Dimension of a sample (#features)\n",
    "## a After transormation, we get nmf_features, whose number os equal to the number of components created.\n",
    "## Original samples can be regenerated by using the components and nmf feature values.\n",
    "## Can build component wise bar graphs of the feature score on that component.\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "model = NMF(n_components=2)\n",
    "\n",
    "model.fit(samples)\n",
    "nmf_features = model.transform(samples) ## .fit_transform()\n",
    "model.components_\n",
    "\n",
    "## Can use cosine similarity to measure the similarities between samples. Such as in topic modeling\n",
    "## Check datacamp slides for this\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2713bcc",
   "metadata": {},
   "source": [
    "### Tree Based Learning\n",
    "\n",
    "DataCamp slides for more and some theory on this including Dos and Donts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ef76410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=2, random_state=1) ## criterion='gini'/'impurity'\n",
    "dt.fit(X_train,y_train)\n",
    "\n",
    "y_pred = dt.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "#### Decision Tree Regressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=4,min_samples_leaf=0.1,random_state=3)\n",
    "\n",
    "dt.fit(X_train, y_train)\n",
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "\n",
    "\n",
    "#### Ensemble Learning\n",
    "\n",
    "\n",
    "#### Voting Classifier\n",
    "\n",
    "## Import models, including VotingClassifier meta-model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "knn = KNN()\n",
    "dt = DecisionTreeClassifier(random_state=SEED)\n",
    "\n",
    "# Define a list called classifier that contains the tuples (classifier_name, classifier)\n",
    "classifiers = [('Logistic Regression', lr),('K Nearest Neighbours', knn),('Classification Tree', dt)]\n",
    "\n",
    "vc = VotingClassifier(estimators=classifiers)\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred = vc.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#### Bagging - Bootstrap Aggregation\n",
    "\n",
    "## Sampling with replacement - to get bootstrap samples. Train the n individual regressors on these bootstrap \n",
    "## samples subset. Meta model is used to get the final prediction.\n",
    "\n",
    "# Import models and utility functions\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.16, random_state=SEED)\n",
    "\n",
    "# Instantiate a BaggingClassifier 'bc'\n",
    "bc = BaggingClassifier(base_estimator=dt, n_estimators=300, n_jobs=-1) \n",
    "# oob_score=True, for OOB Score. (samples not used to training the individual estimator)\n",
    "\n",
    "bc.fit(X_train, y_train) ## This trains multiple estimators on the bootstrap samples.\n",
    "y_pred = bc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Random Forests -- For definitions check the DataCamp slides\n",
    "\n",
    "## Creates botstrap samples but each estimator gets as many bootstrapped samples as the size of the original dataset\n",
    "## Random Forest is a special case of bagging where featured are sampled as well (per estimator).\n",
    "\n",
    "\n",
    "## RF Regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=400,min_samples_leaf=0.12,random_state=SEED)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "## Feature Importance\n",
    "importances_rf = pd.Series(rf.feature_importances_, index = X.columns)\n",
    "sorted_importances_rf = importances_rf.sort_values()\n",
    "sorted_importances_rf.plot(kind='barh', color='lightgreen'); plt.show()\n",
    "\n",
    "\n",
    "## RF Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "#### Boosting\n",
    "\n",
    "#### AdaBoost - Adaptive Boosting\n",
    "## Each predictor pays more attention to the instances wrongly predicted by its predecessor\n",
    "## Achieved by changing the weights of training instances.\n",
    "from sklearn.ensemble import AdaBoostClassifier ## AdaBoostRegressor for the regressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=1, random_state=SEED)\n",
    "\n",
    "adb_clf = AdaBoostClassifier(base_estimator=dt, n_estimators=100)\n",
    "\n",
    "adb_clf.fit(X_train, y_train)\n",
    "y_pred_proba = adb_clf.predict_proba(X_test)[:,1]\n",
    "adb_clf_roc_auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "\n",
    "\n",
    "#### Gradient Boosted Trees\n",
    "## Sequential correction of predecessor's errors.\n",
    "## Each predictor is trained using its predecessor's residual errors as labels.\n",
    "## a CART is used as a base learner\n",
    "## Con - May lead to CARTs using the same split points and maybe the same features. SGB helps solve this\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbt = GradientBoostingRegressor(n_estimators=300, max_depth=1, random_state=SEED)\n",
    "\n",
    "gbt.fit(X_train, y_train)\n",
    "y_pred = gbt.predict(X_test)\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "\n",
    "\n",
    "#### Stochastic Gradient Boosting (SGB)\n",
    "## Each tree is trained on a sample of the data (without replacement, 40-80% of training set)\n",
    "## Features are sampled (without replacement) when choosing split points.\n",
    "## Further ensemble diversity that is it adds further variance to the ensemble of trees\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "sgbt = GradientBoostingRegressor(max_depth=1, subsample=0.8, max_features=0.2, n_estimators=300, random_state=SEED)\n",
    "\n",
    "sgbt.fit(X_train, y_train)\n",
    "y_pred = sgbt.predict(X_test)\n",
    "\n",
    "rmse_test = MSE(y_test, y_pred)**(1/2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### CART Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f082c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4144a367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c4fbda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdfe916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e10cb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91503818",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbc1099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Pipelining\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [('imputation', imp),('logistic_regression', logreg)] ## imp is the sklearn imputer logreg the model instantiated\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "pipeline.fit() --> pipeline.predict() --> pipeline.score\n",
    "\n",
    "\n",
    "#### Pipeline with GridSearch\n",
    "\n",
    "\n",
    "parameters = {'knn__n_neighbors': np.arange(1, 50)} ## key  = step name + \"__\" parameter name\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)\n",
    "cv.fit(X_train, y_train)\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "cv.best_params_\n",
    "cv.score(X_test, y_test)\n",
    "classification_report(y_test, y_pred) ## From metrics\n",
    "\n",
    "\n",
    "#### FunctionTransformer() & FeatureUnion() Example\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "get_text_data = FunctionTransformer(lambda x: x['text'], validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x[['numeric','with_missing']], validate=False)\n",
    "## text, numeric and with_missing are just column names.\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "## This basically, concatenates the features given for union, side by side.\n",
    "\n",
    "union = FeatureUnion([ ('numeric', numeric_pipeline), ('text', text_pipeline) ])\n",
    "\n",
    "numeric_pipeline = Pipeline([('selector', get_numeric_data),('imputer', Imputer())])\n",
    "text_pipeline = Pipeline([('selector', get_text_data),('vectorizer', CountVectorizer())])\n",
    "\n",
    "## Remove backslashes before getting the below in one line\n",
    "pl = Pipeline([\\\n",
    "               ('union', FeatureUnion([\\\n",
    "                                       ('numeric_features', Pipeline([('selector', get_numeric_data),('imputer', Imputer())])),\\\n",
    "                                       ('text_features', Pipeline([('selector', get_text_data),('vectorizer', CountVectorizer())]))\\\n",
    "                                      ])\\\n",
    "               ),('clf', OneVsRestClassifier(LogisticRegression()))])\n",
    "\n",
    "\n",
    "pl = Pipeline([('union', FeatureUnion([('numeric', numeric_pipeline),('text', text_pipeline)])),('clf', OneVsRestClassifier(LogisticRegression()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebc4f29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "languages = ('French', 'Spanish')\n",
    "my_languages = ['English']\n",
    "my_languages = my_languages.extend(\"ABC\")\n",
    "print(my_languages)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
